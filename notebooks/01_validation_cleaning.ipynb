{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b90d8a3-a819-4955-a7f4-e7294490ed15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# =============================================================================\n",
    "# RUTAS DEL REPOSITORIO\n",
    "# - Estructura esperada:\n",
    "#   repo/\n",
    "#     data/        (entrada, dataset anonimizado)\n",
    "#     outputs/     (salidas generadas por notebooks)\n",
    "#     notebooks/   (este notebook)\n",
    "# =============================================================================\n",
    "NOTEBOOK_DIR = Path.cwd().resolve()\n",
    "REPO_ROOT = NOTEBOOK_DIR.parent\n",
    "\n",
    "DATA_DIR = REPO_ROOT / \"data\"\n",
    "LOCAL_INPUT = DATA_DIR / \"validations_anon.csv\"\n",
    "\n",
    "# Fuente alternativa para ejecución en entornos sin el CSV local\n",
    "REMOTE_INPUT = (\n",
    "    \"https://raw.githubusercontent.com/\"\n",
    "    \"diferviec/transport-data-analysis/main/data/validations_anon.csv\"\n",
    ")\n",
    "\n",
    "OUT_DIR = REPO_ROOT / \"outputs\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "OUT_FILE = OUT_DIR / \"validations_clean.csv\"\n",
    "\n",
    "\n",
    "def read_input(local_path: Path, remote_url: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Lectura robusta del dataset:\n",
    "    - Prioriza el archivo local.\n",
    "    - Si no existe, intenta cargar desde el repositorio.\n",
    "    \"\"\"\n",
    "    if local_path.exists():\n",
    "        try:\n",
    "            return pd.read_csv(local_path)\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to read local file: {local_path} | {e}\") from e\n",
    "\n",
    "    try:\n",
    "        return pd.read_csv(remote_url)\n",
    "    except Exception as e:\n",
    "        raise FileNotFoundError(\n",
    "            \"Could not read input data from either:\\n\"\n",
    "            f\"- Local:  {local_path}\\n\"\n",
    "            f\"- Remote: {remote_url}\\n\"\n",
    "            f\"Error: {e}\"\n",
    "        ) from e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f872e007-601e-44da-ad3b-cf8b14708018",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DATETIME\n",
    "# Motivo:\n",
    "# - En operación se observa ruido típico de exportaciones\n",
    "#   y variaciones del separador.\n",
    "# =============================================================================\n",
    "def _clean_invisibles(s: pd.Series) -> pd.Series:\n",
    "    return (\n",
    "        s.astype(str).fillna(\"\")\n",
    "         .str.replace(\"\\ufeff\", \"\", regex=False)   # BOM\n",
    "         .str.replace(\"\\u00A0\", \" \", regex=False)  # NBSP\n",
    "         .str.replace(\"\\u200b\", \"\", regex=False)   # zero-width\n",
    "         .str.replace(\"\\t\", \"\", regex=False)\n",
    "         .str.replace(\"\\r\", \"\", regex=False)\n",
    "         .str.replace(\"\\n\", \"\", regex=False)\n",
    "         .str.strip()\n",
    "    )\n",
    "\n",
    "def _sanitize_iso_datetime_str(s: pd.Series) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Extrae un datetime ISO desde texto “sucio”.\n",
    "    Acepta:\n",
    "      - YYYY-MM-DDTHH:MM:SS\n",
    "      - YYYY-MM-DDTHH:MM:SS.sss...\n",
    "      - YYYY-MM-DD HH:MM:SS(.sss...)  -> normaliza a 'T'\n",
    "    \"\"\"\n",
    "    s = _clean_invisibles(s)\n",
    "\n",
    "    # Consistencia: espacio -> 'T'\n",
    "    s2 = s.str.replace(\" \", \"T\", regex=False)\n",
    "\n",
    "    # Extracción estricta del patrón ISO\n",
    "    iso = s2.str.extract(\n",
    "        r\"(\\d{4}-\\d{2}-\\d{2}T\\d{2}:\\d{2}:\\d{2}(?:\\.\\d{1,6})?)\",\n",
    "        expand=False\n",
    "    )\n",
    "\n",
    "    # Si no coincide, se conserva el texto ya limpiado\n",
    "    return iso.fillna(s2)\n",
    "\n",
    "\n",
    "def parse_datetime_determinista(dt_raw: pd.Series) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Parseo determinista:\n",
    "    - Limpia caracteres invisibles\n",
    "    - Normaliza a ISO con 'T'\n",
    "    - Parsea con formato fijo con/sin milisegundos\n",
    "    \"\"\"\n",
    "    s = _sanitize_iso_datetime_str(dt_raw)\n",
    "    has_ms = s.str.contains(r\"\\.\", regex=True, na=False)\n",
    "\n",
    "    dt = pd.Series(pd.NaT, index=s.index, dtype=\"datetime64[ns]\")\n",
    "\n",
    "    dt.loc[has_ms] = pd.to_datetime(\n",
    "        s.loc[has_ms],\n",
    "        format=\"%Y-%m-%dT%H:%M:%S.%f\",\n",
    "        errors=\"coerce\",\n",
    "    )\n",
    "    dt.loc[~has_ms] = pd.to_datetime(\n",
    "        s.loc[~has_ms],\n",
    "        format=\"%Y-%m-%dT%H:%M:%S\",\n",
    "        errors=\"coerce\",\n",
    "    )\n",
    "\n",
    "    return dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68b01813-0e2d-4b2b-b6e3-3faae5440a8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input rows: 18,564\n",
      "Final rows: 14,224\n",
      "Min DateTime: 2026-01-20 05:31:45.800000\n",
      "Max DateTime: 2026-01-20 21:26:56.579000\n",
      "Saved cleaned dataset to: C:\\Users\\DiegoEveraldoFernand\\Python\\Repositorio1\\outputs\\validations_clean.csv\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CARGA Y VALIDACIONES MÍNIMAS DE ESQUEMA\n",
    "# =============================================================================\n",
    "df_raw = read_input(LOCAL_INPUT, REMOTE_INPUT)\n",
    "\n",
    "required_cols = {\n",
    "    \"DateTime\",\n",
    "    \"SupportId\",\n",
    "    \"StopPlaceShortName\",\n",
    "    \"TransactionType\",\n",
    "    \"ProfileName\",\n",
    "    \"EquipmentModel\",\n",
    "    \"ValidationStatus\",\n",
    "    \"ValidationTicket\",\n",
    "}\n",
    "missing = required_cols - set(df_raw.columns)\n",
    "if missing:\n",
    "    raise ValueError(f\"Missing required columns: {sorted(missing)}\")\n",
    "\n",
    "df = df_raw.copy()\n",
    "\n",
    "# =============================================================================\n",
    "# NORMALIZACIÓN TEMPORAL\n",
    "# - Mantiene únicamente registros con DateTime parseable.\n",
    "# - Ordena para procesos posteriores basados en secuencia cronológica.\n",
    "# =============================================================================\n",
    "df[\"DateTime\"] = parse_datetime_determinista(df[\"DateTime\"])\n",
    "df = df[df[\"DateTime\"].notna()].copy()\n",
    "df = df.sort_values(\"DateTime\").reset_index(drop=True)\n",
    "\n",
    "# =============================================================================\n",
    "# FILTROS OPERATIVOS (REGLAS DE NEGOCIO)\n",
    "# - ValidationStatus == OK: transacción aceptada a nivel operativo.\n",
    "# - ValidationTicket == RETURN_CODE_OK: validación correcta del ticket/soporte.\n",
    "# - Se excluye actividad de agente (operación interna / pruebas / soporte).\n",
    "# =============================================================================\n",
    "df = df[df[\"ValidationStatus\"] == \"OK\"].copy()\n",
    "df = df[df[\"ValidationTicket\"] == \"RETURN_CODE_OK\"].copy()\n",
    "df = df[~df[\"ProfileName\"].isin([\"Agente\"])].copy()\n",
    "\n",
    "# =============================================================================\n",
    "# NORMALIZACIÓN DE ESTACIÓN\n",
    "# - Homologa variaciones por codificación o captura del nombre.\n",
    "# - \"Bus Durán\" se alinea a la estación \"Durán\".\n",
    "# =============================================================================\n",
    "df[\"StopPlaceShortName\"] = (\n",
    "    df[\"StopPlaceShortName\"]\n",
    "      .astype(str)\n",
    "      .str.strip()\n",
    "      .replace({\n",
    "          \"Bus Durán\": \"Durán\",\n",
    "          \"DurÃ¡n\": \"Durán\",\n",
    "      })\n",
    ")\n",
    "\n",
    "# =============================================================================\n",
    "# AGRUPACIÓN DE PERFIL\n",
    "# - Simplifica el análisis: Estandar vs Preferencial.\n",
    "# - Cualquier perfil distinto de Estandar se colapsa a Preferencial.\n",
    "# =============================================================================\n",
    "df[\"ProfileName\"] = df[\"ProfileName\"].where(\n",
    "    df[\"ProfileName\"] == \"Estandar\",\n",
    "    \"Preferencial\"\n",
    ")\n",
    "\n",
    "# =============================================================================\n",
    "# ALCANCE DEL DATASET PARA VIAJES\n",
    "# - En esta etapa se excluye la \"Validadora Bus\" para mantener lógica basada en GATE.\n",
    "# =============================================================================\n",
    "df = df[df[\"EquipmentModel\"] != \"Validadora Bus\"].copy()\n",
    "\n",
    "# Columnas usadas solo como filtros (se retiran para la salida final)\n",
    "df = df.drop(columns=[c for c in [\"ValidationStatus\", \"ValidationTicket\"] if c in df.columns])\n",
    "\n",
    "# =============================================================================\n",
    "# NORMALIZACIÓN DE TransactionType\n",
    "# - Estandariza para lógica downstream (ENTRY/EXIT).\n",
    "# - CORRESPONDENCE_ENTRY se trata como ENTRY para consistencia de eventos.\n",
    "# =============================================================================\n",
    "df[\"TransactionType\"] = (\n",
    "    df[\"TransactionType\"]\n",
    "      .astype(str)\n",
    "      .str.strip()\n",
    "      .str.upper()\n",
    "      .replace({\n",
    "          \"CORRESPONDENCE_ENTRY\": \"ENTRY\",\n",
    "      })\n",
    ")\n",
    "\n",
    "# =============================================================================\n",
    "# HIGIENE DE CAMPOS CLAVE\n",
    "# - Elimina espacios y normaliza nulos típicos de lectura CSV.\n",
    "# - Evita inconsistencias en joins/agrupaciones posteriores.\n",
    "# =============================================================================\n",
    "for c in [\"SupportId\", \"StopPlaceShortName\", \"TransactionType\", \"ProfileName\", \"EquipmentModel\"]:\n",
    "    if c in df.columns:\n",
    "        df[c] = df[c].astype(str).str.strip()\n",
    "        df[c] = df[c].replace({\"nan\": pd.NA, \"None\": pd.NA, \"\": pd.NA})\n",
    "\n",
    "# Orden final para análisis por tarjeta/soporte\n",
    "df = df.sort_values([\"SupportId\", \"DateTime\"]).reset_index(drop=True)\n",
    "\n",
    "# Métricas rápidas para control del resultado\n",
    "print(\"Input rows:\", f\"{len(df_raw):,}\")\n",
    "print(\"Final rows:\", f\"{len(df):,}\")\n",
    "print(\"Min DateTime:\", df[\"DateTime\"].min())\n",
    "print(\"Max DateTime:\", df[\"DateTime\"].max())\n",
    "\n",
    "df.to_csv(OUT_FILE, index=False, encoding=\"utf-8\")\n",
    "print(f\"Saved cleaned dataset to: {OUT_FILE.resolve()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
